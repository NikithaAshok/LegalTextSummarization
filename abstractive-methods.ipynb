{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikithaAshok/LegalTextSummarization/blob/main/abstractive-methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNoQKsPyuAoi",
        "outputId": "017b50a1-d0b9-4736-a63f-fd0dd7966a1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKN8HY-H6F7f",
        "outputId": "d4c83902-c10f-405d-d54f-3f1a70d57255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from PyPDF2) (4.5.0)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0MftYvN6rOY",
        "outputId": "510f00f5-e071-42db-9f4b-01f43ac0748d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n"
      ],
      "metadata": {
        "id": "CrBW7x1NXyYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bb9SrpBBoY7",
        "outputId": "d6a164b3-4300-4cad-e5cc-2e59cbe34863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#T5 is an encoder-decoder model. Converts all language problems into text-to-text format.\n",
        "\n",
        "# from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
        "import transformers\n",
        "# transformers.cache.clear_cache()\n",
        "import PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import string\n",
        "\n",
        "#creating a pdf file object\n",
        "pdf = open(\"/content/drive/MyDrive/example.pdf\",\"rb\")\n",
        "\n",
        "#creating a pdf reader object\n",
        "pdf_reader = PyPDF2.PdfReader(pdf)\n",
        "\n",
        "#checking number of pages in a pdf file\n",
        "#print(\"Number of pages in the pdf \",len(pdf_reader.pages))\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "#stemming tokens \n",
        "\n",
        "#function for tokenization and removing stopwords\n",
        "def remove_stop_words(text):\n",
        "    punct_removed_text = text.translate(str.maketrans('','',string.punctuation))\n",
        "    words = nltk.word_tokenize(punct_removed_text)\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "#function for stemming\n",
        "def stemming(text):\n",
        "    tokens = text.split(' ')\n",
        "\n",
        "    #defining a Stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    #stem the tokens \n",
        "    stemmed_tokens = []\n",
        "\n",
        "\n",
        "    for token in tokens:\n",
        "        stemmed_token = stemmer.stem(token)\n",
        "        stemmed_tokens.append(stemmed_token)\n",
        "\n",
        "    #join the stemmed tokens back into a string\n",
        "    stemmed_text = ' '.join(stemmed_tokens)\n",
        "\n",
        "    return stemmed_text\n",
        "\n",
        "#T5 Model \n",
        "# t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "# t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "#BART Model\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "for i in range(0,len(pdf_reader.pages)):\n",
        "    page = pdf_reader.pages[i]\n",
        "    extracted_text = page.extract_text() # extraction happens here\n",
        "    text_without_stopwords = remove_stop_words(extracted_text) #removing stopwords (function called)\n",
        "    sentences = sent_tokenize(extracted_text)\n",
        "    text = \"summarize:\" + extracted_text\n",
        "\n",
        "#     #T5 MODEL -----------\n",
        "#     #converting input sequence to input-ids through process of encoding - encode()\n",
        "#     input_ids = t5_tokenizer.encode(text,return_tensors='pt',max_length=512)\n",
        "#     #generate() method returns a sequence of ids corresponding to the summary\n",
        "#     summary_ids = t5_model.generate(input_ids)\n",
        "#     #using decode() function to generate summary text from the above ids\n",
        "#     t5_summary = t5_tokenizer.decode(summary_ids[0])\n",
        "#     print(t5_summary)\n",
        "\n",
        "    #BART MODEL ------------\n",
        "    bart_inputs = bart_tokenizer.encode(extracted_text,return_tensors='pt', max_length=1024, truncation=True)\n",
        "    bart_summary_ids = bart_model.generate(bart_inputs, num_beams=4, max_length=100, early_stopping=True)\n",
        "#     bart_summary = bart_tokenizer.decode(bart_summary_ids[0],skip_special_tokens=True)\n",
        "    bart_summary = bart_tokenizer.decode(bart_summary_ids[0],skip_special_tokens=True)\n",
        "    print('summary',bart_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laOLIejR61ga",
        "outputId": "65cf66fe-24ff-4a2a-a198-ce61754583e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summary Self Attention Based Legal Document Summarization- A Technical Review. The study was conducted at the Atria Institute of Technology, Bangalore. The author is Nikitha A. A. Jain, a professor of computer science at the institute. The report was published in the online edition of the ATSR.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8yooNoYUffio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OO8415xqffMw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}